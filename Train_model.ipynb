{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79d99ef"
      },
      "source": [
        "# Tworzenie modelu TTS\n",
        "\n"
      ],
      "id": "f79d99ef"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA7u44acwNRS",
        "outputId": "abae95eb-a944-4d63-daf1-6a4a489261d8"
      },
      "id": "tA7u44acwNRS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa2aec78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89eb8f92-e554-44fb-f2a0-da3ae7dfb749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.21.3-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.6)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.11.4)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.1.0+cu118)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.1)\n",
            "Collecting scikit-learn>=1.3.0 (from TTS)\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.66.1)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (23.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.5)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.7.1)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.33-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.49.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.6.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops>=0.6.0 (from TTS)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.35.2)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS) (3.6.1)\n",
            "Collecting numpy==1.22.0 (from TTS)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.58.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.13.1)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (4.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS)\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.0.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.0->TTS) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.3.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20230927-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (1.12)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (2.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (2.14.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.4.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask>=2.0.1->TTS) (2.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.5.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer>=0.0.32->TTS) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer>=0.0.32->TTS) (3.2.2)\n",
            "Downloading TTS-0.21.3-cp310-cp310-manylinux1_x86_64.whl (942 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m942.6/942.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.33-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20230927-py3-none-any.whl (71.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/71.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, umap-learn, bnnumerizer, bnunicodenormalizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75792 sha256=06e5fb8f83fa8a50fb6983e5f05af7f37290f55adf5d4ed5f678fce4eb99fa11\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=62155843279b99c1c72e306e678dafea18d7a21e88c755a7c2717c1bbc5d646e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=4880eb8bd25ec0578cee5e5f589eae682c5b72d8429a32981fd56a4055ad909b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5259 sha256=d791afc1f8cda05c9b9c776c820237939092c5853e98d81f98f22c7999ccc1cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.6-py3-none-any.whl size=22779 sha256=8a6d55cb4ee8826a675d1a1dced33dd7dd3b3af152bf69b73b5837e062016760\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/d7/e9/16732a619cbf5a63fdc9f6e2f9eb5fcf73fa023735237330e9\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=81751406fb6bf11b1f3a8f6e9380a3fc84d906c4ce1f5f367c755a3771c1c8e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=83749495ffe1f18e0df748350d87994a35a9ae8931d49f91475a7eeb12b9e153\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498182 sha256=0dcb086a8b298f9b0be9cb7f23fbdca12a9f58485804ebe58c4a1c1bcca6e3fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297178 sha256=239932fbdaa9eff266d8053097c33b79246cb095a89b8e50eae833772e06be3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173796 sha256=dd84de251e5d29ef9ce37c8dc4ee60d7f7d969cec3eb8363fdd19b08b2726d5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/0a/90/788d92c07744b329b9283e37b29b064f5db6b1bb0442a1a19b\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=6bf32e730a4cbd2d8e0468b61568307e0cdd1c4d4f288bd8b6e8bdd3923f96ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "Successfully built gruut encodec umap-learn bnnumerizer bnunicodenormalizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, python-crfsuite, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, pysbd, pypinyin, numpy, num2words, networkx, jsonlines, gruut-ipa, einops, coqpit, anyascii, g2pkk, dateparser, scikit-learn, gruut, pynndescent, librosa, encodec, umap-learn, trainer, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.1\n",
            "    Uninstalling librosa-0.10.1:\n",
            "      Successfully uninstalled librosa-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.21.3 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.6 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 einops-0.7.0 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 gruut_lang_es-2.0.0 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 pynndescent-0.5.11 pypinyin-0.49.0 pysbd-0.3.4 python-crfsuite-0.9.9 scikit-learn-1.3.2 sudachidict-core-20230927 sudachipy-0.6.7 trainer-0.0.33 umap-learn-0.5.5 unidecode-1.3.7\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Install Coqui TTS\n",
        "! pip install -U pip\n",
        "! pip install TTS"
      ],
      "id": "fa2aec78"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f226c8-4e55-48fa-937b-8415d539b17c"
      },
      "source": [
        "## Przygotowanie datasetu\n",
        "\n"
      ],
      "id": "e7f226c8-4e55-48fa-937b-8415d539b17c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/original/tts_train_dir\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n"
      ],
      "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae6b7019-3685-4b48-8917-c152e288d7e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b588ebdf-a30f-4304-c032-940faabaae42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-06 01:36:27--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "Resolving data.keithito.com (data.keithito.com)... 24.199.73.137\n",
            "Connecting to data.keithito.com (data.keithito.com)|24.199.73.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748572632 (2.6G) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/original/tts_train_dir/LJSpeech-1.1.tar.bz2’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   2.56G  67.8MB/s    in 37s     \n",
            "\n",
            "2023-12-06 01:37:05 (70.1 MB/s) - ‘/content/drive/MyDrive/original/tts_train_dir/LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and extract LJSpeech dataset.\n",
        "\n",
        "!wget -O $output_path/LJSpeech-1.1.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "!tar -xf $output_path/LJSpeech-1.1.tar.bz2 -C $output_path"
      ],
      "id": "ae6b7019-3685-4b48-8917-c152e288d7e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872eabe1-c36f-45e4-defc-9d6caef9f110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseDatasetConfig(formatter='ljspeech', dataset_name='', path='/content/drive/MyDrive/original/tts_train_dir/LJSpeech-1.1/', meta_file_train='metadata.csv', ignored_speakers=None, language='', phonemizer='', meta_file_val='', meta_file_attn_mask='')\n"
          ]
        }
      ],
      "source": [
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"LJSpeech-1.1/\")\n",
        ")\n",
        "print(dataset_config)"
      ],
      "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae82fd75"
      },
      "source": [
        "## Trening Modelu"
      ],
      "id": "ae82fd75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f"
      },
      "source": [
        "Ustawienie konfiguracji\n"
      ],
      "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84"
      },
      "outputs": [],
      "source": [
        "# GlowTTSConfig: all model related values for training, validating and testing.\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=100,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    save_step=1000,\n",
        ")"
      ],
      "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
      },
      "source": [
        "Inicjalizacja Audio Procesora"
      ],
      "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
        "outputId": "a5a613ba-4313-411d-af41-9a24d0de5ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:True\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:45\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n"
          ]
        }
      ],
      "source": [
        "from TTS.utils.audio import AudioProcessor\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "# Modify sample rate if for a custom audio dataset:\n",
        "# ap.sample_rate = 22050\n"
      ],
      "id": "b1b12f61-f851-4565-84dd-7640947e04ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "014879b7-f18d-44c0-b24a-e10f8002113a"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)"
      ],
      "id": "014879b7-f18d-44c0-b24a-e10f8002113a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
        "outputId": "012fbcc8-9982-4d62-ed4b-86b338bc71c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Found 13100 files in /content/drive/MyDrive/original/tts_train_dir/LJSpeech-1.1\n"
          ]
        }
      ],
      "source": [
        "from TTS.tts.datasets import load_tts_samples\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ],
      "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
      },
      "source": [
        "Inicjalizacja modelu"
      ],
      "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)"
      ],
      "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
        "outputId": "8df54c26-6242-4668-c3e3-9a4dca07a8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: True\n",
            " | > Precision: fp16\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 54321\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            " > Model has 28610257 parameters\n"
          ]
        }
      ],
      "source": [
        "from trainer import Trainer, TrainerArgs\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")"
      ],
      "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
      },
      "source": [
        "### Rozpoczecie treningu\n",
        "\n",
        "\n"
      ],
      "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
        "outputId": "e929e582-af82-4578-8a75-83723c379191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Pre-computing phonemes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/12969 [00:01<1:52:47,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ɪnstɛd əv weɪtɪŋ ðɛɹ, ɔzwɔld əpɛɹəntli wɛnt æz fɑɹ əweɪ æz hi kʊd ænd bɔɹdɪd ðə fɚst oʊk klɪf bʌs wɪt͡ʃ keɪm əlɔŋ\n",
            " [!] Character '͡' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 2059/12969 [01:55<06:56, 26.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n",
            " [!] Character '“' not found in the vocabulary. Discarding it.\n",
            "ɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n",
            " [!] Character '”' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12969/12969 [08:13<00:00, 26.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 01:53:19) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:53:36 -- STEP: 0/406 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 10.6426  (10.642576694488525)\n",
            "     | > loader_time: 5.6152  (5.615214109420776)\n",
            "\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:53:57 -- STEP: 25/406 -- GLOBAL_STEP: 25\u001b[0m\n",
            "     | > loss: 3.6917357444763184  (3.590440209706624)\n",
            "     | > log_mle: 0.7658892869949341  (0.7655702948570251)\n",
            "     | > loss_dur: 2.9258463382720947  (2.8248698870340982)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.3731, device='cuda:0')  (tensor(9.4708, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.7976  (0.8526643180847168)\n",
            "     | > loader_time: 0.0175  (7.738290567398071)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:54:22 -- STEP: 50/406 -- GLOBAL_STEP: 50\u001b[0m\n",
            "     | > loss: 3.725149393081665  (3.5774156033992766)\n",
            "     | > log_mle: 0.7631301879882812  (0.766794255375862)\n",
            "     | > loss_dur: 2.962019205093384  (2.810621345043182)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.5052, device='cuda:0')  (tensor(9.8923, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.1569  (0.9116265344619751)\n",
            "     | > loader_time: 0.0049  (3.8762847232818602)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:54:47 -- STEP: 75/406 -- GLOBAL_STEP: 75\u001b[0m\n",
            "     | > loss: 3.6595194339752197  (3.575093753521259)\n",
            "     | > log_mle: 0.7713882923126221  (0.7668976343595062)\n",
            "     | > loss_dur: 2.8881311416625977  (2.808196119161752)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.3994, device='cuda:0')  (tensor(10.0017, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.8597  (0.9294756889343262)\n",
            "     | > loader_time: 0.0064  (2.5872539679209394)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:55:16 -- STEP: 100/406 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss: 3.5454869270324707  (3.5704586029052736)\n",
            "     | > log_mle: 0.7709070444107056  (0.7669073773754967)\n",
            "     | > loss_dur: 2.7745797634124756  (2.8035512288411457)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.1806, device='cuda:0')  (tensor(10.0504, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.9202  (0.9898034787178039)\n",
            "     | > loader_time: 0.011  (1.943315072059631)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:55:44 -- STEP: 125/406 -- GLOBAL_STEP: 125\u001b[0m\n",
            "     | > loss: 3.582847833633423  (3.5653940947159475)\n",
            "     | > log_mle: 0.7680084705352783  (0.7666726070901622)\n",
            "     | > loss_dur: 2.8148393630981445  (2.7987214917721954)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.1679, device='cuda:0')  (tensor(10.0708, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.8334  (1.0084022502899168)\n",
            "     | > loader_time: 0.0171  (1.5570212345123289)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:56:19 -- STEP: 150/406 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss: 3.5398974418640137  (3.5654702884810314)\n",
            "     | > log_mle: 0.7697139978408813  (0.7664887568780354)\n",
            "     | > loss_dur: 2.770183563232422  (2.798981533731733)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.1140, device='cuda:0')  (tensor(10.0901, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.0648  (1.0707496213912961)\n",
            "     | > loader_time: 0.0053  (1.2996496677398686)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:56:52 -- STEP: 175/406 -- GLOBAL_STEP: 175\u001b[0m\n",
            "     | > loss: 3.5237345695495605  (3.562782560695301)\n",
            "     | > log_mle: 0.7653003334999084  (0.7663370558709809)\n",
            "     | > loss_dur: 2.758434295654297  (2.7964455084367232)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.0071, device='cuda:0')  (tensor(10.0948, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.9629  (1.105750863211495)\n",
            "     | > loader_time: 0.0067  (1.1157939079829626)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:57:27 -- STEP: 200/406 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss: 3.4799318313598633  (3.561329650878906)\n",
            "     | > log_mle: 0.7683870196342468  (0.7661788887099216)\n",
            "     | > loss_dur: 2.7115447521209717  (2.7951507643649465)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.9516, device='cuda:0')  (tensor(10.0987, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.0339  (1.1392060244083406)\n",
            "     | > loader_time: 0.014  (0.9782135891914365)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:58:03 -- STEP: 225/406 -- GLOBAL_STEP: 225\u001b[0m\n",
            "     | > loss: 3.5429418087005615  (3.5576017989668736)\n",
            "     | > log_mle: 0.7625045776367188  (0.7660973565523015)\n",
            "     | > loss_dur: 2.7804372310638428  (2.791504442969035)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.0415, device='cuda:0')  (tensor(10.0946, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.2412  (1.1704429456922738)\n",
            "     | > loader_time: 0.0095  (0.8709911812676321)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:58:40 -- STEP: 250/406 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss: 3.6468265056610107  (3.556425130367279)\n",
            "     | > log_mle: 0.7646295428276062  (0.7660183807214102)\n",
            "     | > loss_dur: 2.8821969032287598  (2.7904067496458698)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.2445, device='cuda:0')  (tensor(10.0943, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.2187  (1.2002115135192866)\n",
            "     | > loader_time: 0.0285  (0.7853246641159055)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 01:59:20 -- STEP: 275/406 -- GLOBAL_STEP: 275\u001b[0m\n",
            "     | > loss: 3.5388050079345703  (3.554982650504922)\n",
            "     | > log_mle: 0.7674552798271179  (0.7657128113620686)\n",
            "     | > loss_dur: 2.7713496685028076  (2.7892698395927007)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.0454, device='cuda:0')  (tensor(10.0920, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.4378  (1.2324984437769106)\n",
            "     | > loader_time: 0.0424  (0.7160192108154295)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:00:02 -- STEP: 300/406 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss: 3.550870895385742  (3.5532740165447367)\n",
            "     | > log_mle: 0.7634040117263794  (0.7654909304503735)\n",
            "     | > loss_dur: 2.7874670028686523  (2.787783086710964)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.0755, device='cuda:0')  (tensor(10.0881, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 2.3918  (1.2681867273648575)\n",
            "     | > loader_time: 0.0505  (0.658629430135091)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:00:46 -- STEP: 325/406 -- GLOBAL_STEP: 325\u001b[0m\n",
            "     | > loss: 3.5352585315704346  (3.5506016299838112)\n",
            "     | > log_mle: 0.7633748650550842  (0.7652398843613878)\n",
            "     | > loss_dur: 2.771883726119995  (2.785361745622425)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(10.0150, device='cuda:0')  (tensor(10.0799, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.5992  (1.303148631315964)\n",
            "     | > loader_time: 0.0148  (0.6099826695368838)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:01:36 -- STEP: 350/406 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss: 3.4455032348632812  (3.549943710074705)\n",
            "     | > log_mle: 0.7596612572669983  (0.7650177950368204)\n",
            "     | > loss_dur: 2.6858420372009277  (2.7849259173168877)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.8005, device='cuda:0')  (tensor(10.0736, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 2.0839  (1.3510841240201659)\n",
            "     | > loader_time: 0.0298  (0.568911222730364)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:02:21 -- STEP: 375/406 -- GLOBAL_STEP: 375\u001b[0m\n",
            "     | > loss: 3.527111768722534  (3.5461661887495484)\n",
            "     | > log_mle: 0.7568140029907227  (0.7647206174184196)\n",
            "     | > loss_dur: 2.7702977657318115  (2.7814455724742335)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.9328, device='cuda:0')  (tensor(10.0612, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.3695  (1.3790158398946102)\n",
            "     | > loader_time: 0.017  (0.5332000319163003)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:03:10 -- STEP: 400/406 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss: 3.4514505863189697  (3.542100526125003)\n",
            "     | > log_mle: 0.7593026757240295  (0.7645237263960714)\n",
            "     | > loss_dur: 2.692147970199585  (2.7775767998817664)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.7264, device='cuda:0')  (tensor(10.0472, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.057  (1.411959818601606)\n",
            "     | > loader_time: 0.006  (0.5012720173597337)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.00674670934677124 \u001b[0m(+0)\n",
            "     | > avg_loss: 3.513145923614502 \u001b[0m(+0)\n",
            "     | > avg_log_mle: 0.759749561548233 \u001b[0m(+0)\n",
            "     | > avg_loss_dur: 2.7533963918685913 \u001b[0m(+0)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/best_model_406.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 02:03:40) \u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:04:04 -- STEP: 19/406 -- GLOBAL_STEP: 425\u001b[0m\n",
            "     | > loss: 3.4858903884887695  (3.5221318571191085)\n",
            "     | > log_mle: 0.7617048025131226  (0.7579264076132524)\n",
            "     | > loss_dur: 2.7241857051849365  (2.764205443231683)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.7684, device='cuda:0')  (tensor(9.6743, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.9686  (1.013396601927908)\n",
            "     | > loader_time: 0.0059  (0.004851567117791427)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:04:33 -- STEP: 44/406 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss: 3.433696985244751  (3.4888532161712646)\n",
            "     | > log_mle: 0.7635617852210999  (0.7596812573346224)\n",
            "     | > loss_dur: 2.670135259628296  (2.7291719534180383)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.6492, device='cuda:0')  (tensor(9.6723, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 0.9309  (1.0810079899701206)\n",
            "     | > loader_time: 0.0039  (0.0061928629875183105)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:05:03 -- STEP: 69/406 -- GLOBAL_STEP: 475\u001b[0m\n",
            "     | > loss: 3.5453810691833496  (3.4827549595763716)\n",
            "     | > log_mle: 0.7670503854751587  (0.7599353626154471)\n",
            "     | > loss_dur: 2.7783305644989014  (2.7228195909140767)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.8768, device='cuda:0')  (tensor(9.6736, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.0952  (1.1271586763686028)\n",
            "     | > loader_time: 0.0192  (0.006962285525556924)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:05:33 -- STEP: 94/406 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss: 3.4325695037841797  (3.475105884227347)\n",
            "     | > log_mle: 0.7597989439964294  (0.759641355022471)\n",
            "     | > loss_dur: 2.6727705001831055  (2.715464528570785)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.5539, device='cuda:0')  (tensor(9.6594, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.6649  (1.146439701952833)\n",
            "     | > loader_time: 0.0151  (0.008385815518967648)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:06:06 -- STEP: 119/406 -- GLOBAL_STEP: 525\u001b[0m\n",
            "     | > loss: 3.3970484733581543  (3.4665148839229296)\n",
            "     | > log_mle: 0.755712628364563  (0.7594333997293681)\n",
            "     | > loss_dur: 2.641335964202881  (2.7070814801865275)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.3741, device='cuda:0')  (tensor(9.6384, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.4567  (1.1771159592796776)\n",
            "     | > loader_time: 0.0129  (0.008849454527141667)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:06:45 -- STEP: 144/406 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss: 3.4359335899353027  (3.4612527721458015)\n",
            "     | > log_mle: 0.7580841779708862  (0.7590506474177042)\n",
            "     | > loss_dur: 2.677849292755127  (2.7022021197610435)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.4730, device='cuda:0')  (tensor(9.6193, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.8921  (1.2404615283012392)\n",
            "     | > loader_time: 0.0343  (0.010375509659449259)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:07:21 -- STEP: 169/406 -- GLOBAL_STEP: 575\u001b[0m\n",
            "     | > loss: 3.455878257751465  (3.4550350135600074)\n",
            "     | > log_mle: 0.7647943496704102  (0.7588112068599496)\n",
            "     | > loss_dur: 2.6910839080810547  (2.696223802115086)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.4844, device='cuda:0')  (tensor(9.5949, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 2.1381  (1.2630072712192877)\n",
            "     | > loader_time: 0.0321  (0.011312656854031352)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:07:57 -- STEP: 194/406 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss: 3.4404382705688477  (3.449710727966938)\n",
            "     | > log_mle: 0.7587132453918457  (0.758426743991596)\n",
            "     | > loss_dur: 2.681725025177002  (2.6912839830536215)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.4281, device='cuda:0')  (tensor(9.5702, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.1226  (1.2836782072008273)\n",
            "     | > loader_time: 0.0067  (0.012007262288909597)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:08:34 -- STEP: 219/406 -- GLOBAL_STEP: 625\u001b[0m\n",
            "     | > loss: 3.3611860275268555  (3.4425366523603325)\n",
            "     | > log_mle: 0.7584353685379028  (0.7581669394284077)\n",
            "     | > loss_dur: 2.602750539779663  (2.684369708849416)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.3161, device='cuda:0')  (tensor(9.5433, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.1068  (1.303136700364553)\n",
            "     | > loader_time: 0.0305  (0.012645273992460068)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:09:12 -- STEP: 244/406 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss: 3.4095890522003174  (3.438535153865814)\n",
            "     | > log_mle: 0.7527146339416504  (0.7579104883260414)\n",
            "     | > loss_dur: 2.656874418258667  (2.6806246608984283)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.3080, device='cuda:0')  (tensor(9.5202, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.2448  (1.3246051336898184)\n",
            "     | > loader_time: 0.0196  (0.01367591932171681)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:09:55 -- STEP: 269/406 -- GLOBAL_STEP: 675\u001b[0m\n",
            "     | > loss: 3.3765738010406494  (3.433267125409775)\n",
            "     | > log_mle: 0.7576212882995605  (0.7574888540909632)\n",
            "     | > loss_dur: 2.618952512741089  (2.675778267551976)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.2343, device='cuda:0')  (tensor(9.4945, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 2.7236  (1.3562838623514852)\n",
            "     | > loader_time: 0.0409  (0.014871624765786096)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:10:37 -- STEP: 294/406 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss: 3.366589069366455  (3.428929916044482)\n",
            "     | > log_mle: 0.7519428730010986  (0.7571214652385837)\n",
            "     | > loss_dur: 2.6146461963653564  (2.6718084479675817)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.0791, device='cuda:0')  (tensor(9.4680, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.3089  (1.380814609073457)\n",
            "     | > loader_time: 0.0354  (0.015950267817698368)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:11:23 -- STEP: 319/406 -- GLOBAL_STEP: 725\u001b[0m\n",
            "     | > loss: 3.3271193504333496  (3.423144481025147)\n",
            "     | > log_mle: 0.7476828098297119  (0.7567506436270228)\n",
            "     | > loss_dur: 2.5794365406036377  (2.6663938345953966)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.0320, device='cuda:0')  (tensor(9.4383, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 3.0597  (1.413088000306515)\n",
            "     | > loader_time: 0.058  (0.017763063078016332)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:12:11 -- STEP: 344/406 -- GLOBAL_STEP: 750\u001b[0m\n",
            "     | > loss: 3.3574776649475098  (3.419459460779679)\n",
            "     | > log_mle: 0.7502849102020264  (0.7563961004448485)\n",
            "     | > loss_dur: 2.6071927547454834  (2.6630633570427134)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(9.0044, device='cuda:0')  (tensor(9.4123, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 2.0594  (1.4468592121157533)\n",
            "     | > loader_time: 0.0089  (0.01920968224835951)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:12:59 -- STEP: 369/406 -- GLOBAL_STEP: 775\u001b[0m\n",
            "     | > loss: 3.2897329330444336  (3.41397910454086)\n",
            "     | > log_mle: 0.7481790781021118  (0.7559842011792867)\n",
            "     | > loss_dur: 2.5415539741516113  (2.6579949002924987)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.8549, device='cuda:0')  (tensor(9.3822, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.6651  (1.477767263970724)\n",
            "     | > loader_time: 0.0613  (0.020262059803577624)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:13:50 -- STEP: 394/406 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > loss: 3.3175573348999023  (3.4079505501664844)\n",
            "     | > log_mle: 0.7545107007026672  (0.7556292175641514)\n",
            "     | > loss_dur: 2.56304669380188  (2.6523213307869615)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.7821, device='cuda:0')  (tensor(9.3500, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 1.3125  (1.511921276900974)\n",
            "     | > loader_time: 0.0307  (0.020560214967291972)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.006984710693359375 \u001b[0m(+0.00023800134658813477)\n",
            "     | > avg_loss:\u001b[92m 3.320562958717346 \u001b[0m(-0.19258296489715576)\n",
            "     | > avg_log_mle:\u001b[92m 0.7483132779598236 \u001b[0m(-0.011436283588409424)\n",
            "     | > avg_loss_dur:\u001b[92m 2.572249710559845 \u001b[0m(-0.18114668130874634)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/best_model_812.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 02:14:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:14:42 -- STEP: 13/406 -- GLOBAL_STEP: 825\u001b[0m\n",
            "     | > loss: 3.2669517993927  (3.3698046024029074)\n",
            "     | > log_mle: 0.7410008907318115  (0.7470268836388221)\n",
            "     | > loss_dur: 2.5259509086608887  (2.6227777371039758)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.5569, device='cuda:0')  (tensor(8.7048, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 0.8997  (0.9356682850764348)\n",
            "     | > loader_time: 0.004  (0.004521700052114634)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:15:11 -- STEP: 38/406 -- GLOBAL_STEP: 850\u001b[0m\n",
            "     | > loss: 3.322772979736328  (3.313806496168438)\n",
            "     | > log_mle: 0.7481401562690735  (0.7485653852161608)\n",
            "     | > loss_dur: 2.5746328830718994  (2.565241129774796)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.5543, device='cuda:0')  (tensor(8.6127, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 0.9852  (1.060628382783187)\n",
            "     | > loader_time: 0.0069  (0.005926358072381271)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:15:40 -- STEP: 63/406 -- GLOBAL_STEP: 875\u001b[0m\n",
            "     | > loss: 3.245422840118408  (3.2930351893107095)\n",
            "     | > log_mle: 0.7425692081451416  (0.748407314694117)\n",
            "     | > loss_dur: 2.5028536319732666  (2.5446278897542807)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.3634, device='cuda:0')  (tensor(8.5443, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.6974  (1.108019881778293)\n",
            "     | > loader_time: 0.0168  (0.007281659141419426)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:16:09 -- STEP: 88/406 -- GLOBAL_STEP: 900\u001b[0m\n",
            "     | > loss: 3.2200536727905273  (3.2760453468019306)\n",
            "     | > log_mle: 0.7476959228515625  (0.7479437305168672)\n",
            "     | > loss_dur: 2.472357749938965  (2.5281016257676208)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.2543, device='cuda:0')  (tensor(8.4698, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.1806  (1.1176088045943866)\n",
            "     | > loader_time: 0.0053  (0.008156077428297564)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:16:45 -- STEP: 113/406 -- GLOBAL_STEP: 925\u001b[0m\n",
            "     | > loss: 3.20037579536438  (3.255104875142595)\n",
            "     | > log_mle: 0.7441062927246094  (0.7472612430564071)\n",
            "     | > loss_dur: 2.4562695026397705  (2.5078436399983084)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(8.0341, device='cuda:0')  (tensor(8.3800, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.3059  (1.1857814873214312)\n",
            "     | > loader_time: 0.0119  (0.010168200045560318)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:17:19 -- STEP: 138/406 -- GLOBAL_STEP: 950\u001b[0m\n",
            "     | > loss: 3.2211227416992188  (3.24061796285104)\n",
            "     | > log_mle: 0.7399781942367554  (0.7463078360626663)\n",
            "     | > loss_dur: 2.481144666671753  (2.494310130243715)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.9280, device='cuda:0')  (tensor(8.2979, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.2332  (1.210624610168346)\n",
            "     | > loader_time: 0.0054  (0.011123624400816105)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:17:54 -- STEP: 163/406 -- GLOBAL_STEP: 975\u001b[0m\n",
            "     | > loss: 3.1321864128112793  (3.2284262648389377)\n",
            "     | > log_mle: 0.7391849756240845  (0.7453736600700331)\n",
            "     | > loss_dur: 2.3930013179779053  (2.483052607694285)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.6511, device='cuda:0')  (tensor(8.2197, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.7178  (1.2379874758925171)\n",
            "     | > loader_time: 0.0195  (0.01195757666979831)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:18:31 -- STEP: 188/406 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > loss: 3.2334187030792236  (3.2147506602266995)\n",
            "     | > log_mle: 0.7371706962585449  (0.7444469202706154)\n",
            "     | > loss_dur: 2.4962480068206787  (2.4703037459799573)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.6192, device='cuda:0')  (tensor(8.1326, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.9838  (1.2635584219973137)\n",
            "     | > loader_time: 0.0089  (0.012825130148136871)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/checkpoint_1000.pth\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:19:13 -- STEP: 213/406 -- GLOBAL_STEP: 1025\u001b[0m\n",
            "     | > loss: 3.066462993621826  (3.2014236573322274)\n",
            "     | > log_mle: 0.7346364259719849  (0.7435325207844586)\n",
            "     | > loss_dur: 2.331826686859131  (2.4578911418646143)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.2829, device='cuda:0')  (tensor(8.0464, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.0298  (1.2868453348186648)\n",
            "     | > loader_time: 0.0055  (0.013424599114718013)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:19:52 -- STEP: 238/406 -- GLOBAL_STEP: 1050\u001b[0m\n",
            "     | > loss: 3.126645565032959  (3.189449393448709)\n",
            "     | > log_mle: 0.7355950474739075  (0.7427493689440879)\n",
            "     | > loss_dur: 2.3910505771636963  (2.446700032017811)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.2703, device='cuda:0')  (tensor(7.9618, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.2052  (1.3108951795000985)\n",
            "     | > loader_time: 0.0113  (0.014032228654172241)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:20:33 -- STEP: 263/406 -- GLOBAL_STEP: 1075\u001b[0m\n",
            "     | > loss: 3.060588836669922  (3.178851308931416)\n",
            "     | > log_mle: 0.732742965221405  (0.7417304674482165)\n",
            "     | > loss_dur: 2.327845811843872  (2.4371208482822078)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(7.0140, device='cuda:0')  (tensor(7.8783, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 2.4618  (1.3386934601308724)\n",
            "     | > loader_time: 0.0302  (0.014585615564208522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:21:15 -- STEP: 288/406 -- GLOBAL_STEP: 1100\u001b[0m\n",
            "     | > loss: 3.0216636657714844  (3.168710863424671)\n",
            "     | > log_mle: 0.7252374887466431  (0.7406906187534332)\n",
            "     | > loss_dur: 2.296426296234131  (2.428020248810449)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.7681, device='cuda:0')  (tensor(7.7940, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.1478  (1.367298522757159)\n",
            "     | > loader_time: 0.021  (0.015315429204040103)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:22:02 -- STEP: 313/406 -- GLOBAL_STEP: 1125\u001b[0m\n",
            "     | > loss: 3.0353593826293945  (3.159331068825035)\n",
            "     | > log_mle: 0.7260209321975708  (0.7397118366945284)\n",
            "     | > loss_dur: 2.3093385696411133  (2.4196192365104)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.6321, device='cuda:0')  (tensor(7.7100, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 2.3405  (1.4045171775756926)\n",
            "     | > loader_time: 0.0206  (0.01577911590234921)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:22:46 -- STEP: 338/406 -- GLOBAL_STEP: 1150\u001b[0m\n",
            "     | > loss: 3.0656487941741943  (3.1510752291369015)\n",
            "     | > log_mle: 0.7249131202697754  (0.7386289401167242)\n",
            "     | > loss_dur: 2.340735673904419  (2.412446293605148)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.5530, device='cuda:0')  (tensor(7.6279, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.3359  (1.430561525581856)\n",
            "     | > loader_time: 0.0285  (0.01615306188368938)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:23:33 -- STEP: 363/406 -- GLOBAL_STEP: 1175\u001b[0m\n",
            "     | > loss: 3.030679225921631  (3.1432301893050347)\n",
            "     | > log_mle: 0.722173273563385  (0.7375389705347949)\n",
            "     | > loss_dur: 2.3085060119628906  (2.4056912217258395)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.4141, device='cuda:0')  (tensor(7.5468, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 2.6385  (1.4606465196478131)\n",
            "     | > loader_time: 0.0284  (0.01655437729575416)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:24:21 -- STEP: 388/406 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > loss: 2.9881114959716797  (3.1351260254063558)\n",
            "     | > log_mle: 0.7232511639595032  (0.7364797165098879)\n",
            "     | > loss_dur: 2.2648603916168213  (2.398646311661629)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.1683, device='cuda:0')  (tensor(7.4663, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 2.3871  (1.4860498733127234)\n",
            "     | > loader_time: 0.0178  (0.01687651381050187)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.007891803979873657 \u001b[0m(+0.0009070932865142822)\n",
            "     | > avg_loss:\u001b[92m 2.9907763302326202 \u001b[0m(-0.32978662848472595)\n",
            "     | > avg_log_mle:\u001b[92m 0.7172440364956856 \u001b[0m(-0.03106924146413803)\n",
            "     | > avg_loss_dur:\u001b[92m 2.27353236079216 \u001b[0m(-0.29871734976768494)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/best_model_1218.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 02:25:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:25:17 -- STEP: 7/406 -- GLOBAL_STEP: 1225\u001b[0m\n",
            "     | > loss: 3.026080846786499  (3.112809113093785)\n",
            "     | > log_mle: 0.7211859226226807  (0.718170131955828)\n",
            "     | > loss_dur: 2.3048949241638184  (2.39463894707816)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.0849, device='cuda:0')  (tensor(6.2423, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 0.8326  (1.1962013925824846)\n",
            "     | > loader_time: 0.0035  (0.0051767826080322266)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:25:43 -- STEP: 32/406 -- GLOBAL_STEP: 1250\u001b[0m\n",
            "     | > loss: 3.1380743980407715  (3.061107374727726)\n",
            "     | > log_mle: 0.7174652814865112  (0.7189185414463282)\n",
            "     | > loss_dur: 2.4206089973449707  (2.3421888425946236)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(6.2617, device='cuda:0')  (tensor(6.1391, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.0534  (1.0480518639087673)\n",
            "     | > loader_time: 0.0048  (0.004820875823497773)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:26:12 -- STEP: 57/406 -- GLOBAL_STEP: 1275\u001b[0m\n",
            "     | > loss: 3.0203194618225098  (3.0411954386192455)\n",
            "     | > log_mle: 0.7165433168411255  (0.718080408740462)\n",
            "     | > loss_dur: 2.303776264190674  (2.3231150434728254)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.8912, device='cuda:0')  (tensor(6.0515, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.0257  (1.1023873571763956)\n",
            "     | > loader_time: 0.0047  (0.006035235890170984)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:26:44 -- STEP: 82/406 -- GLOBAL_STEP: 1300\u001b[0m\n",
            "     | > loss: 2.9729385375976562  (3.0316791185518586)\n",
            "     | > log_mle: 0.7122718691825867  (0.7164872470425396)\n",
            "     | > loss_dur: 2.260666608810425  (2.31519187659752)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.7152, device='cuda:0')  (tensor(5.9842, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 0.9792  (1.1418469620913996)\n",
            "     | > loader_time: 0.0048  (0.007238297927670363)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:27:19 -- STEP: 107/406 -- GLOBAL_STEP: 1325\u001b[0m\n",
            "     | > loss: 2.9374611377716064  (3.0219690420917256)\n",
            "     | > log_mle: 0.7070309519767761  (0.7146353721618653)\n",
            "     | > loss_dur: 2.2304301261901855  (2.3073336677016507)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.6264, device='cuda:0')  (tensor(5.9218, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.0624  (1.20052494066898)\n",
            "     | > loader_time: 0.0188  (0.009320987719241702)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:27:52 -- STEP: 132/406 -- GLOBAL_STEP: 1350\u001b[0m\n",
            "     | > loss: 3.045103073120117  (3.0154241775021404)\n",
            "     | > log_mle: 0.6964486837387085  (0.7124992839314722)\n",
            "     | > loss_dur: 2.3486545085906982  (2.3029248949253205)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.6383, device='cuda:0')  (tensor(5.8617, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.1002  (1.2226549964962588)\n",
            "     | > loader_time: 0.0152  (0.010416877992225414)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:28:28 -- STEP: 157/406 -- GLOBAL_STEP: 1375\u001b[0m\n",
            "     | > loss: 3.1096084117889404  (3.0139655262042004)\n",
            "     | > log_mle: 0.6989517211914062  (0.7106509694627895)\n",
            "     | > loss_dur: 2.410656690597534  (2.3033145582600016)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.6880, device='cuda:0')  (tensor(5.8141, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.0408  (1.2519128808550022)\n",
            "     | > loader_time: 0.0057  (0.011445394746816842)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:29:04 -- STEP: 182/406 -- GLOBAL_STEP: 1400\u001b[0m\n",
            "     | > loss: 3.00083065032959  (3.010180796895709)\n",
            "     | > log_mle: 0.6894159317016602  (0.7086734424580584)\n",
            "     | > loss_dur: 2.3114147186279297  (2.301507356402639)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.4696, device='cuda:0')  (tensor(5.7644, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.2588  (1.2755072431249939)\n",
            "     | > loader_time: 0.0203  (0.012517693278553723)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:29:41 -- STEP: 207/406 -- GLOBAL_STEP: 1425\u001b[0m\n",
            "     | > loss: 2.9978227615356445  (3.008254928865295)\n",
            "     | > log_mle: 0.6891700625419617  (0.7067909594895183)\n",
            "     | > loss_dur: 2.308652639389038  (2.3014639713913914)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.4089, device='cuda:0')  (tensor(5.7225, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.1441  (1.297568583833999)\n",
            "     | > loader_time: 0.0093  (0.012939509562248196)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:30:20 -- STEP: 232/406 -- GLOBAL_STEP: 1450\u001b[0m\n",
            "     | > loss: 2.9737842082977295  (3.005565030821439)\n",
            "     | > log_mle: 0.6817142367362976  (0.7048889755688864)\n",
            "     | > loss_dur: 2.292069911956787  (2.300676058078633)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.3504, device='cuda:0')  (tensor(5.6844, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.5352  (1.3213033357570916)\n",
            "     | > loader_time: 0.0077  (0.013478153738482247)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:31:02 -- STEP: 257/406 -- GLOBAL_STEP: 1475\u001b[0m\n",
            "     | > loss: 2.962064504623413  (3.0046819722142204)\n",
            "     | > log_mle: 0.6815164089202881  (0.7029402070472213)\n",
            "     | > loss_dur: 2.280548095703125  (2.301741768877793)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.2884, device='cuda:0')  (tensor(5.6515, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.1492  (1.3532972317261458)\n",
            "     | > loader_time: 0.0136  (0.014277045364973612)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:31:42 -- STEP: 282/406 -- GLOBAL_STEP: 1500\u001b[0m\n",
            "     | > loss: 2.9373202323913574  (3.002310823041497)\n",
            "     | > log_mle: 0.6804660558700562  (0.7009457320186263)\n",
            "     | > loss_dur: 2.256854295730591  (2.3013650956728773)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.1870, device='cuda:0')  (tensor(5.6193, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.2807  (1.3756769109279554)\n",
            "     | > loader_time: 0.0144  (0.014628910849280391)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:32:28 -- STEP: 307/406 -- GLOBAL_STEP: 1525\u001b[0m\n",
            "     | > loss: 2.95963454246521  (3.000468243993455)\n",
            "     | > log_mle: 0.6748029589653015  (0.6989816324330309)\n",
            "     | > loss_dur: 2.2848315238952637  (2.301486614861006)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.2063, device='cuda:0')  (tensor(5.5903, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 2.5443  (1.4085855351985477)\n",
            "     | > loader_time: 0.0272  (0.015326913870895336)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:33:13 -- STEP: 332/406 -- GLOBAL_STEP: 1550\u001b[0m\n",
            "     | > loss: 2.9465184211730957  (2.997530716011324)\n",
            "     | > log_mle: 0.6687657833099365  (0.696984786226089)\n",
            "     | > loss_dur: 2.277752637863159  (2.3005459337349383)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.1946, device='cuda:0')  (tensor(5.5633, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.3232  (1.4379921691963484)\n",
            "     | > loader_time: 0.0089  (0.015879729426050766)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:34:01 -- STEP: 357/406 -- GLOBAL_STEP: 1575\u001b[0m\n",
            "     | > loss: 2.917396068572998  (2.9962085962963383)\n",
            "     | > log_mle: 0.6713074445724487  (0.6949492344001426)\n",
            "     | > loss_dur: 2.2460885047912598  (2.301259366738027)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.1415, device='cuda:0')  (tensor(5.5400, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.9149  (1.4686658896651936)\n",
            "     | > loader_time: 0.0363  (0.01655018630147983)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:34:48 -- STEP: 382/406 -- GLOBAL_STEP: 1600\u001b[0m\n",
            "     | > loss: 2.932732582092285  (2.992220162097073)\n",
            "     | > log_mle: 0.663483202457428  (0.6929453051215064)\n",
            "     | > loss_dur: 2.269249439239502  (2.2992748621246566)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.1416, device='cuda:0')  (tensor(5.5155, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 2.8123  (1.4932532235589964)\n",
            "     | > loader_time: 0.0373  (0.017098808787880157)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.006057292222976685 \u001b[0m(-0.0018345117568969727)\n",
            "     | > avg_loss:\u001b[92m 2.8965582847595215 \u001b[0m(-0.09421804547309875)\n",
            "     | > avg_log_mle:\u001b[92m 0.6587095484137535 \u001b[0m(-0.05853448808193207)\n",
            "     | > avg_loss_dur:\u001b[92m 2.2378487586975098 \u001b[0m(-0.03568360209465027)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/best_model_1624.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 02:35:49) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:35:55 -- STEP: 1/406 -- GLOBAL_STEP: 1625\u001b[0m\n",
            "     | > loss: 2.964743137359619  (2.964743137359619)\n",
            "     | > log_mle: 0.6646634340286255  (0.6646634340286255)\n",
            "     | > loss_dur: 2.300079584121704  (2.300079584121704)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.2000, device='cuda:0')  (tensor(5.2000, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.3158  (1.3157923221588135)\n",
            "     | > loader_time: 0.0053  (0.005329132080078125)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:36:22 -- STEP: 26/406 -- GLOBAL_STEP: 1650\u001b[0m\n",
            "     | > loss: 2.882145643234253  (2.9283294952832737)\n",
            "     | > log_mle: 0.6635565161705017  (0.6661727978633001)\n",
            "     | > loss_dur: 2.2185890674591064  (2.2621566974199734)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.1102, device='cuda:0')  (tensor(5.1567, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 0.9266  (1.0674278552715597)\n",
            "     | > loader_time: 0.0043  (0.004836761034451998)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:36:52 -- STEP: 51/406 -- GLOBAL_STEP: 1675\u001b[0m\n",
            "     | > loss: 2.8779866695404053  (2.9034159370497163)\n",
            "     | > log_mle: 0.6529541015625  (0.6643137066972022)\n",
            "     | > loss_dur: 2.2250325679779053  (2.239102242039699)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.0423, device='cuda:0')  (tensor(5.1161, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.1337  (1.1235345064425002)\n",
            "     | > loader_time: 0.0077  (0.005377376780790439)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:37:22 -- STEP: 76/406 -- GLOBAL_STEP: 1700\u001b[0m\n",
            "     | > loss: 2.865143299102783  (2.8942800666156567)\n",
            "     | > log_mle: 0.6609855890274048  (0.6615473722156725)\n",
            "     | > loss_dur: 2.204157590866089  (2.2327327069483296)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.0580, device='cuda:0')  (tensor(5.1027, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.6787  (1.150975017171157)\n",
            "     | > loader_time: 0.0159  (0.0063573498474924165)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:37:57 -- STEP: 101/406 -- GLOBAL_STEP: 1725\u001b[0m\n",
            "     | > loss: 2.883845329284668  (2.881538707431)\n",
            "     | > log_mle: 0.6449857950210571  (0.6586554823535504)\n",
            "     | > loss_dur: 2.2388596534729004  (2.2228832339296245)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(5.0955, device='cuda:0')  (tensor(5.0857, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.6931  (1.2052282555268543)\n",
            "     | > loader_time: 0.0134  (0.007168783999905726)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:38:30 -- STEP: 126/406 -- GLOBAL_STEP: 1750\u001b[0m\n",
            "     | > loss: 2.7550911903381348  (2.868519762205699)\n",
            "     | > log_mle: 0.6410640478134155  (0.6556391266603319)\n",
            "     | > loss_dur: 2.114027261734009  (2.2128806473716853)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.8881, device='cuda:0')  (tensor(5.0639, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.4966  (1.2229854473992)\n",
            "     | > loader_time: 0.0204  (0.008133825801667709)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:39:04 -- STEP: 151/406 -- GLOBAL_STEP: 1775\u001b[0m\n",
            "     | > loss: 2.785989999771118  (2.8597265489843506)\n",
            "     | > log_mle: 0.6399703025817871  (0.6527000681453983)\n",
            "     | > loss_dur: 2.146019697189331  (2.2070264895230727)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.9140, device='cuda:0')  (tensor(5.0468, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.4998  (1.245287895202637)\n",
            "     | > loader_time: 0.0063  (0.009166419111340246)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:39:41 -- STEP: 176/406 -- GLOBAL_STEP: 1800\u001b[0m\n",
            "     | > loss: 2.7652292251586914  (2.849776808511127)\n",
            "     | > log_mle: 0.6310330629348755  (0.6499350856650963)\n",
            "     | > loss_dur: 2.1341960430145264  (2.1998417323285877)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.8555, device='cuda:0')  (tensor(5.0280, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 2.0117  (1.2725566463036968)\n",
            "     | > loader_time: 0.018  (0.010414456779306582)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:40:18 -- STEP: 201/406 -- GLOBAL_STEP: 1825\u001b[0m\n",
            "     | > loss: 2.725771427154541  (2.8391357179897936)\n",
            "     | > log_mle: 0.6316031813621521  (0.6472353193890402)\n",
            "     | > loss_dur: 2.094168186187744  (2.1919004063108067)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.8094, device='cuda:0')  (tensor(5.0072, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 2.1604  (1.2978607410222143)\n",
            "     | > loader_time: 0.0197  (0.011497424016544475)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:40:56 -- STEP: 226/406 -- GLOBAL_STEP: 1850\u001b[0m\n",
            "     | > loss: 2.724390983581543  (2.8278890974753725)\n",
            "     | > log_mle: 0.6177122592926025  (0.6445839587038602)\n",
            "     | > loss_dur: 2.1066787242889404  (2.1833051432550485)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.7816, device='cuda:0')  (tensor(4.9866, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.3639  (1.3196237941758822)\n",
            "     | > loader_time: 0.0208  (0.012469644040133046)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:41:35 -- STEP: 251/406 -- GLOBAL_STEP: 1875\u001b[0m\n",
            "     | > loss: 2.7585105895996094  (2.8188918320781204)\n",
            "     | > log_mle: 0.6157799959182739  (0.642008800668071)\n",
            "     | > loss_dur: 2.142730474472046  (2.1768830344971444)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.8360, device='cuda:0')  (tensor(4.9697, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.3729  (1.3393804200616966)\n",
            "     | > loader_time: 0.0191  (0.013013951806908109)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:42:15 -- STEP: 276/406 -- GLOBAL_STEP: 1900\u001b[0m\n",
            "     | > loss: 2.6832823753356934  (2.8094305240589637)\n",
            "     | > log_mle: 0.6119852662086487  (0.6393286570690682)\n",
            "     | > loss_dur: 2.0712971687316895  (2.1701018689335263)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.7323, device='cuda:0')  (tensor(4.9527, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.7637  (1.3624388117721113)\n",
            "     | > loader_time: 0.0299  (0.013470203116320181)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:42:59 -- STEP: 301/406 -- GLOBAL_STEP: 1925\u001b[0m\n",
            "     | > loss: 2.7308568954467773  (2.8000886448197964)\n",
            "     | > log_mle: 0.6029200553894043  (0.6367871117750274)\n",
            "     | > loss_dur: 2.127936840057373  (2.1633015360151027)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.7751, device='cuda:0')  (tensor(4.9346, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.2913  (1.3914515180049145)\n",
            "     | > loader_time: 0.0194  (0.014153509837052358)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:43:47 -- STEP: 326/406 -- GLOBAL_STEP: 1950\u001b[0m\n",
            "     | > loss: 2.6225786209106445  (2.7896869943185796)\n",
            "     | > log_mle: 0.5999613404273987  (0.634247158568329)\n",
            "     | > loss_dur: 2.0226173400878906  (2.1554398383099618)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.6310, device='cuda:0')  (tensor(4.9158, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 2.8772  (1.4312692317494584)\n",
            "     | > loader_time: 0.0163  (0.014756888699677827)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:44:32 -- STEP: 351/406 -- GLOBAL_STEP: 1975\u001b[0m\n",
            "     | > loss: 2.648735284805298  (2.7814830449911256)\n",
            "     | > log_mle: 0.5989232063293457  (0.6316747050679303)\n",
            "     | > loss_dur: 2.049812078475952  (2.149808342979843)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.6532, device='cuda:0')  (tensor(4.8991, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.5008  (1.4538632526017323)\n",
            "     | > loader_time: 0.0214  (0.015313411370301861)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:45:20 -- STEP: 376/406 -- GLOBAL_STEP: 2000\u001b[0m\n",
            "     | > loss: 2.589405059814453  (2.7710015716704897)\n",
            "     | > log_mle: 0.5957258939743042  (0.6291366590464362)\n",
            "     | > loss_dur: 1.9936790466308594  (2.1418649145263284)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.5199, device='cuda:0')  (tensor(4.8797, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.4265  (1.4840495446895032)\n",
            "     | > loader_time: 0.037  (0.01573021234350002)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/checkpoint_2000.pth\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:46:11 -- STEP: 401/406 -- GLOBAL_STEP: 2025\u001b[0m\n",
            "     | > loss: 2.6008074283599854  (2.761022593315104)\n",
            "     | > log_mle: 0.588431179523468  (0.6266986240175294)\n",
            "     | > loss_dur: 2.012376308441162  (2.134323970635334)\n",
            "     | > amp_scaler: 16384.0  (16424.8578553616)\n",
            "     | > grad_norm: tensor(4.5711, device='cuda:0')  (tensor(4.8498, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.0566  (1.506584644912187)\n",
            "     | > loader_time: 0.006  (0.015842425258379624)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.010662168264389038 \u001b[0m(+0.0046048760414123535)\n",
            "     | > avg_loss:\u001b[92m 2.5343425571918488 \u001b[0m(-0.36221572756767273)\n",
            "     | > avg_log_mle:\u001b[92m 0.5874077007174492 \u001b[0m(-0.07130184769630432)\n",
            "     | > avg_loss_dur:\u001b[92m 1.9469348788261414 \u001b[0m(-0.2909138798713684)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000/best_model_2030.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/100\u001b[0m\n",
            " --> /content/drive/MyDrive/original/tts_train_dir/run-December-06-2023_01+44AM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-12-06 02:46:37) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:47:03 -- STEP: 20/406 -- GLOBAL_STEP: 2050\u001b[0m\n",
            "     | > loss: 2.48616361618042  (2.6068166971206663)\n",
            "     | > log_mle: 0.6098449230194092  (0.6022091299295426)\n",
            "     | > loss_dur: 1.8763188123703003  (2.0046075642108914)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.3723, device='cuda:0')  (tensor(4.5662, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 0.9191  (1.011035716533661)\n",
            "     | > loader_time: 0.0044  (0.0051048994064331055)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:47:33 -- STEP: 45/406 -- GLOBAL_STEP: 2075\u001b[0m\n",
            "     | > loss: 2.558176279067993  (2.5724647204081217)\n",
            "     | > log_mle: 0.5985900163650513  (0.5999609669049581)\n",
            "     | > loss_dur: 1.959586262702942  (1.9725037707222832)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.4742, device='cuda:0')  (tensor(4.5060, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 1.2262  (1.1029908180236816)\n",
            "     | > loader_time: 0.0056  (0.005865854687160914)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:48:15 -- STEP: 70/406 -- GLOBAL_STEP: 2100\u001b[0m\n",
            "     | > loss: 2.511697292327881  (2.561521029472351)\n",
            "     | > log_mle: 0.5802531242370605  (0.5958792584283009)\n",
            "     | > loss_dur: 1.9314441680908203  (1.965641781261989)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.3906, device='cuda:0')  (tensor(4.4860, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 1.0129  (1.308785053661891)\n",
            "     | > loader_time: 0.0058  (0.007573185648236956)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:48:50 -- STEP: 95/406 -- GLOBAL_STEP: 2125\u001b[0m\n",
            "     | > loss: 2.4374961853027344  (2.545019543798346)\n",
            "     | > log_mle: 0.5800004005432129  (0.5919665575027466)\n",
            "     | > loss_dur: 1.8574957847595215  (1.9530529925697728)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.3233, device='cuda:0')  (tensor(4.4582, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 1.1027  (1.328787121019865)\n",
            "     | > loader_time: 0.0052  (0.008912109073839691)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:49:23 -- STEP: 120/406 -- GLOBAL_STEP: 2150\u001b[0m\n",
            "     | > loss: 2.4809441566467285  (2.531167809168497)\n",
            "     | > log_mle: 0.5667928457260132  (0.5883304759860039)\n",
            "     | > loss_dur: 1.9141514301300049  (1.9428373366594314)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.3730, device='cuda:0')  (tensor(4.4338, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 1.1449  (1.3229088028271991)\n",
            "     | > loader_time: 0.0087  (0.0093060572942098)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-12-06 02:49:58 -- STEP: 145/406 -- GLOBAL_STEP: 2175\u001b[0m\n",
            "     | > loss: 2.3966095447540283  (2.521016310001242)\n",
            "     | > log_mle: 0.5658800005912781  (0.5845840745958789)\n",
            "     | > loss_dur: 1.830729603767395  (1.9364322407492276)\n",
            "     | > amp_scaler: 16384.0  (16384.0)\n",
            "     | > grad_norm: tensor(4.2145, device='cuda:0')  (tensor(4.4151, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 1.1497  (1.3291023172181229)\n",
            "     | > loader_time: 0.0056  (0.010236578974230536)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainer.fit()"
      ],
      "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f"
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAYK0ZvIJPV7",
        "outputId": "d8713ff5-1e22-4caf-c311-c8530767f7c0"
      },
      "id": "SAYK0ZvIJPV7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "DwBCCPyQJTPe"
      },
      "id": "DwBCCPyQJTPe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/tts_train_dir/run-December-05-2023_03+58PM-0000000/config.json')\n",
        "files.download('/content/tts_train_dir/run-December-05-2023_03+58PM-0000000/best_model.pth')\n",
        "files.download('/content/tts_train_dir/LJSpeech-1.1/metadata.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "920hKv9brlLj",
        "outputId": "20769d9e-c073-481c-f0f8-67e7698a6615"
      },
      "id": "920hKv9brlLj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-422f413895ed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tts_train_dir/run-December-05-2023_03+58PM-0000000/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tts_train_dir/run-December-05-2023_03+58PM-0000000/best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tts_train_dir/LJSpeech-1.1/metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/tts_train_dir/run-December-05-2023_03+58PM-0000000/config.json"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/run-December-05-2023_03+58PM-0000000.zip /content/tts_train_dir/run-December-05-2023_03+58PM-0000000\n",
        "from google.colab import files\n",
        "files.download(\"/content/run-December-05-2023_03+58PM-0000000.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "9GwmfKCgt6gp",
        "outputId": "b10d8e24-f4d0-40b4-a162-49d92ff1cee8"
      },
      "id": "9GwmfKCgt6gp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-548609530ba0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r /content/tts_train_dir/run-December-05-2023_03+58PM-0000000.zip /content/Folder_To_Zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/file.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "!tensorboard --logdir=tts_train_dir"
      ],
      "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6dc959"
      },
      "source": [
        "## Testowanie modelu\n"
      ],
      "id": "9f6dc959"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "output_path = \"tts_train_dir\"\n",
        "ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n",
        "configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])\n"
      ],
      "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd42bc7a"
      },
      "outputs": [],
      "source": [
        " !tts --text \"Text for TTS\" \\\n",
        "      --model_path \"/content/tts_train_dir/run-December-03-2023_05+06PM-0000000/best_model.pth\" \\\n",
        "      --config_path \"/content/tts_train_dir/run-December-03-2023_05+06PM-0000000/config.json\" \\\n",
        "      --out_path out.wav"
      ],
      "id": "dd42bc7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.display.Audio(\"out.wav\")"
      ],
      "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}